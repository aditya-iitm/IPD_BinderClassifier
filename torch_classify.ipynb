{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import os,sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples 4168\n"
     ]
    }
   ],
   "source": [
    "all_ssm_npzs = glob.glob('/net/scratch/aditya20/af2exp/ssm_initaf2_training/*.npz')\n",
    "print(f'Number of training examples {len(all_ssm_npzs)}')\n",
    "import random as rn\n",
    "rn.shuffle(all_ssm_npzs)\n",
    "test_ssm_npzs = all_ssm_npzs[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '/home/aditya20/experimentsWaf2'\n",
    "scratch_path = '/net/scratch/aditya20/af2exp'\n",
    "score_labels = {}\n",
    "with open(f'{home_path}/valid_binder_data.sc') as f:\n",
    "    for line in f:\n",
    "        words = line.strip().split()\n",
    "        pdb = words[0]\n",
    "        beneficial = words[1]\n",
    "        neutral = words[3]\n",
    "        \n",
    "        if beneficial == 'True' or neutral == 'True':\n",
    "            score_labels[pdb] = 1\n",
    "        else:\n",
    "            score_labels[pdb] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "def _jnp_softmax(x, axis=-1):\n",
    "    unnormalized = jnp.exp(x - jax.lax.stop_gradient(x.max(axis, keepdims=True)))\n",
    "    return unnormalized / unnormalized.sum(axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "arrays_list = []\n",
    "labels = []\n",
    "\n",
    "for file in test_ssm_npzs:\n",
    "    try:\n",
    "        npzfile = np.load(file,allow_pickle=True)\n",
    "        mask = npzfile['arr_0'].item()['peptide_mask']\n",
    "        L1 = np.count_nonzero(mask)\n",
    "\n",
    "        pae = npzfile['arr_0'].item()['predicted_aligned_error']\n",
    "        pae_logits = npzfile['arr_0'].item()['pae_logits']\n",
    "        pae_inter = 0.5*(pae[:L1,L1:].mean()+pae[L1:,:L1].mean())\n",
    "\n",
    "        Ca_mask = npzfile['arr_0'].item()['structure_module']['final_atom_mask'][:,1]\n",
    "        Ca_mask_2D = Ca_mask[:,None]*Ca_mask[None,:]\n",
    "        #Ca_mask_2D = np.expand_dims(Ca_mask_2D, axis=-1)\n",
    "\n",
    "        mask_2D = (mask[:,None])*(1-mask[None,:])\n",
    "        mask_2D_symm = mask_2D + mask_2D.T\n",
    "\n",
    "        pae_probs = _jnp_softmax(pae_logits)\n",
    "\n",
    "        #print(Ca_mask_2D.shape, mask_2D_symm.shape)\n",
    "        '''\n",
    "        pae_probs_new = np.zeros((300,300,64))\n",
    "        pae_probs_new[:pae_probs.shape[0],:pae_probs.shape[1]] = pae_probs\n",
    "\n",
    "        Ca_mask_new = np.zeros((300,300,1))\n",
    "        Ca_mask_new[:Ca_mask_2D.shape[0],:Ca_mask_2D.shape[1]] = Ca_mask_2D\n",
    "\n",
    "        mask_new = np.zeros((300,300))\n",
    "        mask_new[:mask_2D_symm.shape[0],:mask_2D_symm.shape[1]] = mask_2D_symm\n",
    "\n",
    "        '''\n",
    "\n",
    "        pae_new = jnp.sum(Ca_mask_2D[:,:,None]*mask_2D_symm[:,:,None]*pae_probs, (-2, -3))/jnp.sum(Ca_mask_2D * mask_2D_symm)\n",
    "        #print(pae_new.shape)\n",
    "        arrays_list.append(pae_new)\n",
    "        pdb = file.split('tmp_')[1].split('__pred')[0]\n",
    "        labels.append(score_labels[pdb])\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "label_inputs = np.array(labels)   \n",
    "pae_inputs = np.stack(arrays_list)\n",
    "np.save('ssm_inputs.npz',pae=pae_inputs,label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'PAE {pae_inputs.shape} label {label_inputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssmset = dataset(pae_inputs,label_inputs)\n",
    "trainloader1 = DataLoader(ssmset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_shape,32)\n",
    "    self.fc2 = nn.Linear(32,64)\n",
    "    self.fc3 = nn.Linear(64,1)\n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.sigmoid(self.fc3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 2000\n",
    "model = Net(input_shape=pae_inputs.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accur = []\n",
    "for i in range(epochs):\n",
    "  for j,(x_train,y_train) in enumerate(trainloader1):\n",
    "    \n",
    "    #calculate output\n",
    "    output = model(x_train)\n",
    "    print(output)\n",
    " \n",
    "    #calculate loss\n",
    "    loss = loss_fn(output,y_train.reshape(-1,1))\n",
    " \n",
    "    #accuracy\n",
    "    \n",
    "    predicted = model(torch.tensor(pae_inputs,dtype=torch.float32))\n",
    "    acc = (predicted.reshape(-1).detach().numpy().round() == label_inputs).mean()\n",
    "    \n",
    "    #backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  if i%200 == 0:\n",
    "    losses.append(loss)\n",
    "    accur.append(acc)\n",
    "    print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = [i.detach().numpy() for i in losses]\n",
    "plt.plot(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3 (Python 3.8.2)",
   "language": "python",
   "name": "se3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
